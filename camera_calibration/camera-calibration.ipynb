{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camera Calibration\n",
    "\n",
    "### Goal<br>\n",
    "- Learn distortions in camera, intrinsic and extrinsic parameters of camera.\n",
    "- Learn to find these parameters and undistort images.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to radial distortion, straight lines appears to be curved. This effect is more as we move away from the center of image.<br> \n",
    "For example, one image is shown below, where two edges of a chess board are marked with red lines. But you can see that border is not a straight line and doesn’t match with the red line. All the expected straight lines are bulged out.\n",
    "\n",
    "![Radial Distortion](calib_radial.jpg \"Radial Distortion\")\n",
    "\n",
    "\n",
    "**The distortion is solved as:**<br>\n",
    "x<sub>{corrected}</sub> = x( 1 + k<sub>1</sub> r<sup>2</sup> + k<sub>2</sub> r<sup>4</sup> + k<sub>3</sub> r<sup>6</sup>) <br> \n",
    "y<sub>{corrected}</sub> = y( 1 + k<sub>1</sub> r<sup>2</sup> + k<sub>2</sub> r<sup>4</sup> + k<sub>3</sub> r<sup>6</sup>)\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Similarly, another distortion is the **tangential distortion** which occurs because image taking lense is not aligned perfectly parallel to the imaging plane.<br> So some areas in image may look nearer than expected. <br><br>\n",
    "**Tangential distortion is solved as:**<br>\n",
    "x<sub>{corrected}</sub> = x + [ 2p<sub>1</sub>xy + p<sub>2</sub>(r<sup>2</sup> + 2x<sup>2</sup>)]\n",
    "<br> \n",
    "y<sub>{corrected}</sub> = y + [p<sub>1</sub>(r<sup>2</sup>+ 2y<sup>2</sup>) +2 p<sub>2</sub>xy]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In short, we need to find five parameters, known as distortion coefficients given by:<br>\n",
    "\n",
    "Distortion coefficients=(k<sub>1</sub>, k<sub>2</sub>, p<sub>1</sub>, p<sub>2</sub>, k<sub>3</sub>)<br>\n",
    "\n",
    "In addition to this, we need to find a few more informatios like:<br> \n",
    "- Intrinsic parameters\n",
    "- Extrinsic parameters of a camera.\n",
    "<br>\n",
    "Intrinsic parameters are specific to a camera. It includes information like focal length (f<sub>x</sub>,f<sub>y</sub>), optical centers (c<sub>x</sub>, c<sub>y</sub>) etc.<br> <br>\n",
    "It is also called camera matrix and depends on the camera only, so once calculated, it can be stored for future purposes. It is expressed as a 3x3 matrix:\n",
    "\n",
    "**camera matrix** = $$\n",
    "\\left(\\begin{array}{cc} \n",
    "f_x & 0 & c_x\\\\\n",
    "0 & f_y & c_y\\\\\n",
    "0 & 0 & 1\n",
    "\\end{array}\\right)\n",
    "$$ \n",
    "\n",
    "Extrinsic parameters corresponds to rotation and translation vectors which translates a coordinates of a 3D point to a coordinate system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For stereo applications, these distortions need to be corrected first. To find all these parameters, what we have to do is to provide some sample images of a well defined pattern (eg, chess board). <br>We find some specific points in it ( square corners in chess board). <br>We know its coordinates in real world space and we know its coordinates in image. For better results, we need atleast 10 test patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Consider just one image of a chess board. Important input datas needed for camera calibration is a set of 3D real world points and its corresponding 2D image points. 2D image points are OK which we can easily find from the image. (These image points are locations where two black squares touch each other in chess boards)\n",
    "\n",
    ">What about the 3D points from real world space? Those images are taken from a static camera and chess boards are placed at different locations and orientations. So we need to know (X,Y,Z) values. But for simplicity, we can say chess board was kept stationary at XY plane, (so Z=0 always) and camera was moved accordingly. This consideration helps us to find only X,Y values. Now for X,Y values, we can simply pass the points as (0,0), (1,0), (2,0), ... which denotes the location of points. In this case, the results we get will be in the scale of size of chess board square. But if we know the square size, (say 30 mm), and we can pass the values as (0,0),(30,0),(60,0),..., we get the results in mm. (In this case, we don’t know square size since we didn’t take those images, so we pass in terms of square size).\n",
    "\n",
    "3D points are called object points and 2D image points are called image points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Packages ##\n",
    "\n",
    "First, let's run the cell below to import all the packages for camera calibration.\n",
    "- [numpy] The fundamental package for scientific computing with Python.\n",
    "- [cv2]   The computer vision 3 package in Python.\n",
    "- [glob]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This folder has sample chessboards images so we will use them\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next write the termination conditions, define object as numpy array and load the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# termination criteria\n",
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
    "\n",
    "# prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)\n",
    "objp = np.zeros((6*7,3), np.float32)\n",
    "objp[:,:2] = np.mgrid[0:7,0:6].T.reshape(-1,2)\n",
    "\n",
    "# Arrays to store object points and image points from all the images.\n",
    "objpoints = [] # 3d point in real world space\n",
    "imgpoints = [] # 2d points in image plane.\n",
    "\n",
    "images = glob.glob('*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mpimg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-836751d8bb90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'img'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mplotimg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmpimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mimgplot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplotimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestroyAllWindows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mpimg' is not defined"
     ]
    }
   ],
   "source": [
    "# We use the images and then we convert them from 3 channel BGR to Gray.\n",
    "for fname in images:\n",
    "    img = cv2.imread(fname)\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Next task is to find the chess board corners.\n",
    "    ret, corners = cv2.findChessboardCorners(gray, (7,6),None)\n",
    "\n",
    "    # If found then we add object points, image points after refining them\n",
    "    if ret == True:\n",
    "        objpoints.append(objp)\n",
    "\n",
    "        corners2 = cv2.cornerSubPix(gray,corners,(11,11),(-1,-1),criteria)\n",
    "        imgpoints.append(corners2)\n",
    "        \n",
    "        # Draw and display the corners\n",
    "        img = cv2.drawChessboardCorners(img, (7,6), corners2,ret)\n",
    "        cv2.imshow('img',img)\n",
    "        cv2.waitKey(500)\n",
    "        plotimg=mpimg.imread(img)\n",
    "        imgplot=plt.imshow(plotimg)\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
